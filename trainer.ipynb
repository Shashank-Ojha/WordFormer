{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Shashank-Ojha/WordFormer/blob/main/trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/usr/local/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/usr/local/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ix (/usr/local/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -mportlib-metadata (/usr/local/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rllib3 (/usr/local/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uture (/usr/local/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yglet (/usr/local/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from torch) (3.7.4.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/usr/local/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/usr/local/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ix (/usr/local/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -mportlib-metadata (/usr/local/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rllib3 (/usr/local/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uture (/usr/local/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yglet (/usr/local/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mThe autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1083bf710>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1083bf710>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils.attention_block import SelfAttention\n",
    "from utils.prepare_data import read_file\n",
    "from utils.prepare_data import get_vocab\n",
    "from utils.prepare_data import train_validation_split\n",
    "from utils.prepare_data import get_batch\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text file, convert each token (char) into integers, and split data into train and validation data.\n",
    "text = read_file('shakespeare.txt')\n",
    "\n",
    "# Here are all the unique characters that occur in this text\n",
    "vocab, vocab_size = get_vocab(text)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(vocab) }\n",
    "itos = { i:ch for i,ch in enumerate(vocab) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "train_data, val_data = train_validation_split(data, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Hyperparameters ------\n",
    "\n",
    "# -- Data Breakdown\n",
    "batch_size = 16\n",
    "block_size = 8\n",
    "\n",
    "# -- Network Parameters\n",
    "# Embedding Block\n",
    "num_embeddings = vocab_size\n",
    "embedding_dim = 8\n",
    "\n",
    "# Attention Block\n",
    "kq_dim = 7\n",
    "v_dim = 6\n",
    "\n",
    "# -- Training Parameters\n",
    "learning_rate = 3e-4\n",
    "max_iters = 50000\n",
    "eval_iters = 1000\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both x and y are (batch_size, block_size)\n",
    "x, y = get_batch(train_data, batch_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 65])\n"
     ]
    }
   ],
   "source": [
    "# Define Network\n",
    "\n",
    "# Input Shape (batch_size, block_size). \n",
    "# Note block_size is not a parameters to the model, so it can accept arbitary\n",
    "# length contexts.\n",
    "model = nn.Sequential(\n",
    "    # Transforms shape to (batch_size, block_size, embedding_dim)\n",
    "    nn.Embedding(num_embeddings, embedding_dim),\n",
    "    # Transforms shape to (batch_size, block_size, v_dim)\n",
    "    SelfAttention(embedding_dim, kq_dim, v_dim),\n",
    "    # Transforms shape to (batch_size, block_size, vocab_size)\n",
    "    nn.Linear(v_dim, vocab_size)\n",
    "    \n",
    ")\n",
    "\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step <built-in function iter>: train loss 4.2920, val loss 4.2903\n",
      "step <built-in function iter>: train loss 3.2538, val loss 3.2823\n",
      "step <built-in function iter>: train loss 3.1020, val loss 3.1020\n",
      "step <built-in function iter>: train loss 3.0278, val loss 3.0290\n",
      "step <built-in function iter>: train loss 2.9686, val loss 2.9704\n",
      "step <built-in function iter>: train loss 2.9241, val loss 2.9332\n",
      "step <built-in function iter>: train loss 2.8800, val loss 2.8890\n",
      "step <built-in function iter>: train loss 2.8357, val loss 2.8539\n",
      "step <built-in function iter>: train loss 2.8181, val loss 2.8302\n",
      "step <built-in function iter>: train loss 2.7954, val loss 2.8130\n",
      "step <built-in function iter>: train loss 2.7824, val loss 2.8018\n",
      "step <built-in function iter>: train loss 2.7631, val loss 2.7794\n",
      "step <built-in function iter>: train loss 2.7501, val loss 2.7671\n",
      "step <built-in function iter>: train loss 2.7382, val loss 2.7542\n",
      "step <built-in function iter>: train loss 2.7269, val loss 2.7398\n",
      "step <built-in function iter>: train loss 2.7169, val loss 2.7356\n",
      "step <built-in function iter>: train loss 2.7055, val loss 2.7283\n",
      "step <built-in function iter>: train loss 2.6880, val loss 2.7038\n",
      "step <built-in function iter>: train loss 2.6744, val loss 2.7030\n",
      "step <built-in function iter>: train loss 2.6605, val loss 2.6787\n",
      "step <built-in function iter>: train loss 2.6525, val loss 2.6672\n",
      "step <built-in function iter>: train loss 2.6468, val loss 2.6542\n",
      "step <built-in function iter>: train loss 2.6317, val loss 2.6512\n",
      "step <built-in function iter>: train loss 2.6236, val loss 2.6372\n",
      "step <built-in function iter>: train loss 2.6212, val loss 2.6296\n",
      "step <built-in function iter>: train loss 2.6106, val loss 2.6281\n",
      "step <built-in function iter>: train loss 2.6086, val loss 2.6194\n",
      "step <built-in function iter>: train loss 2.5955, val loss 2.6137\n",
      "step <built-in function iter>: train loss 2.5941, val loss 2.6053\n",
      "step <built-in function iter>: train loss 2.5848, val loss 2.5975\n",
      "step <built-in function iter>: train loss 2.5693, val loss 2.5793\n",
      "step <built-in function iter>: train loss 2.5526, val loss 2.5556\n",
      "step <built-in function iter>: train loss 2.5383, val loss 2.5449\n",
      "step <built-in function iter>: train loss 2.5200, val loss 2.5347\n",
      "step <built-in function iter>: train loss 2.5218, val loss 2.5253\n",
      "step <built-in function iter>: train loss 2.5123, val loss 2.5197\n",
      "step <built-in function iter>: train loss 2.5069, val loss 2.5122\n",
      "step <built-in function iter>: train loss 2.4992, val loss 2.5099\n",
      "step <built-in function iter>: train loss 2.4894, val loss 2.5026\n",
      "step <built-in function iter>: train loss 2.4923, val loss 2.4962\n",
      "step <built-in function iter>: train loss 2.4860, val loss 2.4839\n",
      "step <built-in function iter>: train loss 2.4781, val loss 2.4826\n",
      "step <built-in function iter>: train loss 2.4767, val loss 2.4787\n",
      "step <built-in function iter>: train loss 2.4630, val loss 2.4763\n",
      "step <built-in function iter>: train loss 2.4674, val loss 2.4764\n",
      "step <built-in function iter>: train loss 2.4706, val loss 2.4770\n",
      "step <built-in function iter>: train loss 2.4575, val loss 2.4693\n",
      "step <built-in function iter>: train loss 2.4640, val loss 2.4697\n",
      "step <built-in function iter>: train loss 2.4506, val loss 2.4630\n",
      "step <built-in function iter>: train loss 2.4530, val loss 2.4609\n",
      "step <built-in function iter>: train loss 2.4443, val loss 2.4583\n"
     ]
    }
   ],
   "source": [
    "# -- Training\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    \"\"\"\n",
    "    logits: Shape (B, T, C)\n",
    "    targets: Shape (B, T)\n",
    "    \"\"\"\n",
    "    (B, T, C) = logits.shape\n",
    "    logits = logits.view(B*T, C)\n",
    "    targets = targets.view(B*T)    \n",
    "    # This wants logits to have have (N, C) and targets to have\n",
    "    # shape (N,), where N is the batch size and C is the number of\n",
    "    # classes.\n",
    "    return F.cross_entropy(logits, targets)\n",
    "    \n",
    "@torch.no_grad()\n",
    "def estimate_loss(train_data, val_data, eval_iters, batch_size, block_size):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for dataset_type, dataset in {'train': train_data, 'val': val_data}.items():\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb = get_batch(dataset, batch_size, block_size)\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            losses[k] = loss.item()\n",
    "        out[dataset_type] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Training Loop\n",
    "for i in range(max_iters):\n",
    "    # Shape = (batch_size, block_size)\n",
    "    xb, yb = get_batch(train_data, batch_size, block_size)\n",
    "    \n",
    "    # Zero out the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Shape = (batch_size, block_size, vocab_size)\n",
    "    logits = model(xb)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_fn(logits, yb)\n",
    "    loss.backward()\n",
    "\n",
    "    # Adjust weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Gather data report\n",
    "    if i % eval_iters == 0 or i == max_iters - 1:\n",
    "        eval_interval\n",
    "        losses = estimate_loss(train_data, val_data, eval_iters, batch_size, block_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CCRYDLBMEEOLwOOOR.GORxRRARMLPLCRSCEUONEZNULREENLRTULIAONEE ISICINNENE AR-IUL Ew:\n",
      "ENLMSYsMELTnORy g\n",
      "\n",
      "\n",
      "DAMORAUNAHRARBgNCRARCDYCARVDFEUOENUUSLE ETYLN:\n",
      "FEwDBLINEETI:\n",
      "\n",
      "TNRRJILI:\n",
      "AIOR\n",
      "\n",
      "\n",
      "HIIOR:\n",
      "WTAShn\n",
      "hwhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuurur.urursurrrrr.I.orrrfurrprnrrrl, lrr br Ioc t lfoab,,o tat, y feeotaaaaaoaaea aaat; t  it grr.o.orcsrprrrrxrrr!!!. rve!\n",
      "Buvrm\n",
      "Cenmtly s c f: f tsizt binhset I oct,oooo looo noo  toeofoo s totooo Voeooooooooooo eo y\n"
     ]
    }
   ],
   "source": [
    "def generate(model, context, max_context_size, max_new_tokens):\n",
    "    \"\"\"\n",
    "    context: array tokens with shape (B, t), where t can be any length.\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        sub_context = context if context.shape[1] < max_context_size else context[:, -max_context_size:]\n",
    "        # Shape = (batch_size, block_size, vocab_size)\n",
    "        logits = model(sub_context)\n",
    "        # Get the last character's predictions. Shape = (B, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1) \n",
    "        \n",
    "        # Sample from the distribution. Shape = (B, 1)\n",
    "        preds = torch.multinomial(probs, num_samples=1)   \n",
    "        \n",
    "        # Add to context\n",
    "        context = torch.cat((context, preds), dim=1)\n",
    "    return context\n",
    "        \n",
    "        \n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "new_text = generate(model, context, block_size, max_new_tokens=500)\n",
    "print(decode(new_text[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNodZddeRw76af4YPftC9SV",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
